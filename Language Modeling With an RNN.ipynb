{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train_data = pd.read_csv(\n",
    "    '/kaggle/input/nlp-getting-started/train.csv', \n",
    "    usecols=['text', 'target'], \n",
    "    dtype={'text': str, 'target': np.int64}\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    '/kaggle/input/nlp-getting-started/test.csv', \n",
    "    usecols=['text', 'id'], \n",
    "    dtype={'text': str, 'id': str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " 'Forest fire near La Ronge Sask. Canada'\n",
      " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " '13,000 people receive #wildfires evacuation orders in California '\n",
      " 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(train_data['text'].head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel some training data There are a number of examples in the training dataset that are mislabelled.\n",
    "\n",
    "indicesto0=[4415, 4400, 4399,4403,4397,4396, 4394,4414, 4393,4392,4404,4407,4420,4412,4408,4391,4405,6840,6834,6837,6841,6816,6828,6831,\n",
    "          3913,3914,3936,3921,3941,3937,3938,3136,3133,3930,3933,3924,3917,246,270,266,259,253,251,250,271,6119,6122,6123,6131,6160,\n",
    "          6166,6167,6172,6212,6221,6230,6091,6108,7435,7460,7464,7466,7469,7475,7489,7495,7500,7525,7552,7572,7591,7599]\n",
    "train_data.loc[indicesto0, 'target'] = 0\n",
    "indicesto1 = [601,576,584,608,606,603,592,604,591, 587]\n",
    "train_data.loc[indicesto1, 'target'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['our deed reason #earthquak may allah forgiv us',\n",
       "       'forest fire near la rong sask. canada',\n",
       "       'all resid ask shelter place notifi officers. no evacu shelter place order expect',\n",
       "       '13,000 peopl receiv #wildfir evacu order california',\n",
       "       'just got sent photo rubi #alaska smoke #wildfir pour school'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a few functions to clean the texts (regularized expressions used)\n",
    "def remove_url(sentence):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', sentence)\n",
    "def remove_at(sentence):\n",
    "    url = re.compile(r'@\\S+')\n",
    "    return url.sub(r'', sentence)\n",
    "def remove_html(sentence):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', sentence)\n",
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "def remove_stopwords(sentence):\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "#also shorten words to stems\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_words(sentence):\n",
    "    words = sentence.split()\n",
    "    words = [stemmer.stem(word) for word in words ]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "def clean_text(data):\n",
    "    data['text'] = data['text'].apply(lambda x : remove_url(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_at(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_html(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_emoji(x))\n",
    "    data['text'] = data['text'].apply(lambda x : remove_stopwords(x))\n",
    "    data['text'] = data['text'].apply(lambda x : stem_words(x))\n",
    "    \n",
    "    return data\n",
    "test_data = clean_text(test_data)\n",
    "train_data = clean_text(train_data)\n",
    "train_data['text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now encode the texts to tokens\n",
    "alldata = pd.concat([train_data['text'], test_data['text']],ignore_index=True)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(alldata)\n",
    "encoded_train = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train_data['text']), padding='pre')\n",
    "encoded_test = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_data['text']), padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,  633, 7223,\n",
       "        522,  281,  134, 1704, 2360,   48], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "7608    1\n",
       "7609    1\n",
       "7610    1\n",
       "7611    1\n",
       "7612    1\n",
       "Name: target, Length: 7613, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=train_data['target']\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4380\n",
       "1    3233\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now visualize certain word distribution in the training set and labels\n",
    "label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts=np.unique(encoded_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 0\n",
    "unique, counts=unique[1:], counts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylim(0,130)\n",
    "plt.bar(unique, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now build models\n",
    "#first model contains simple RNN as well as embedding layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary :  16552\n",
      "Maximum length of tweet :  28\n"
     ]
    }
   ],
   "source": [
    "# Vocbvulary Size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Size of Vocabulary : ', vocab_size)\n",
    "# Maximum length for padding sequence\n",
    "maxlen = max(len(x) for x in encoded_train)\n",
    "print('Maximum length of tweet : ', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 100, input_length = maxlen),\n",
    "    keras.layers.SimpleRNN(100,return_sequences=True),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model1.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 28, 100)           1655200   \n",
      "_________________________________________________________________\n",
      "simple_rnn_29 (SimpleRNN)    (None, 28, 100)           20100     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2800)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 2801      \n",
      "=================================================================\n",
      "Total params: 1,678,101\n",
      "Trainable params: 1,678,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-426b2dfe578d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "label=label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4380, 3233]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/50\n",
      "6851/6851 [==============================] - 4s 569us/sample - loss: 0.0398 - accuracy: 0.9829 - val_loss: 1.0358 - val_accuracy: 0.7192\n",
      "Epoch 2/50\n",
      "6851/6851 [==============================] - 4s 587us/sample - loss: 0.0392 - accuracy: 0.9820 - val_loss: 1.0167 - val_accuracy: 0.7310\n",
      "Epoch 3/50\n",
      "6851/6851 [==============================] - 4s 569us/sample - loss: 0.0394 - accuracy: 0.9816 - val_loss: 0.9754 - val_accuracy: 0.7283\n",
      "Epoch 4/50\n",
      "6851/6851 [==============================] - 4s 570us/sample - loss: 0.0381 - accuracy: 0.9831 - val_loss: 1.0907 - val_accuracy: 0.7165\n",
      "Epoch 5/50\n",
      "6851/6851 [==============================] - 4s 570us/sample - loss: 0.0371 - accuracy: 0.9832 - val_loss: 0.9667 - val_accuracy: 0.7270\n",
      "Epoch 6/50\n",
      "6851/6851 [==============================] - 4s 571us/sample - loss: 0.0368 - accuracy: 0.9847 - val_loss: 0.9821 - val_accuracy: 0.7244\n",
      "Epoch 7/50\n",
      "6851/6851 [==============================] - 4s 564us/sample - loss: 0.0364 - accuracy: 0.9828 - val_loss: 1.0798 - val_accuracy: 0.7073\n",
      "Epoch 8/50\n",
      "6851/6851 [==============================] - 4s 559us/sample - loss: 0.0371 - accuracy: 0.9826 - val_loss: 1.0123 - val_accuracy: 0.7178\n",
      "Epoch 9/50\n",
      "6851/6851 [==============================] - 4s 556us/sample - loss: 0.0345 - accuracy: 0.9838 - val_loss: 0.9936 - val_accuracy: 0.7257\n",
      "Epoch 10/50\n",
      "6851/6851 [==============================] - 4s 595us/sample - loss: 0.0337 - accuracy: 0.9842 - val_loss: 1.0258 - val_accuracy: 0.7165\n",
      "Epoch 11/50\n",
      "6851/6851 [==============================] - 4s 584us/sample - loss: 0.0334 - accuracy: 0.9850 - val_loss: 1.0025 - val_accuracy: 0.7192\n",
      "Epoch 12/50\n",
      "6851/6851 [==============================] - 4s 588us/sample - loss: 0.0326 - accuracy: 0.9850 - val_loss: 1.0305 - val_accuracy: 0.7087\n",
      "Epoch 13/50\n",
      "6851/6851 [==============================] - 4s 577us/sample - loss: 0.0330 - accuracy: 0.9838 - val_loss: 1.0351 - val_accuracy: 0.7231\n",
      "Epoch 14/50\n",
      "6851/6851 [==============================] - 4s 560us/sample - loss: 0.0320 - accuracy: 0.9857 - val_loss: 0.9557 - val_accuracy: 0.7257\n",
      "Epoch 15/50\n",
      "6851/6851 [==============================] - 4s 568us/sample - loss: 0.0321 - accuracy: 0.9845 - val_loss: 0.9985 - val_accuracy: 0.7113\n",
      "Epoch 16/50\n",
      "6816/6851 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9847\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "6851/6851 [==============================] - 4s 575us/sample - loss: 0.0320 - accuracy: 0.9848 - val_loss: 1.0235 - val_accuracy: 0.7192\n",
      "Epoch 17/50\n",
      "6851/6851 [==============================] - 4s 566us/sample - loss: 0.0258 - accuracy: 0.9882 - val_loss: 1.0010 - val_accuracy: 0.7231\n",
      "Epoch 18/50\n",
      "6851/6851 [==============================] - 4s 563us/sample - loss: 0.0253 - accuracy: 0.9883 - val_loss: 1.0002 - val_accuracy: 0.7270\n",
      "Epoch 19/50\n",
      "6851/6851 [==============================] - 4s 578us/sample - loss: 0.0251 - accuracy: 0.9882 - val_loss: 1.0039 - val_accuracy: 0.7270\n",
      "Epoch 20/50\n",
      "6851/6851 [==============================] - 4s 571us/sample - loss: 0.0248 - accuracy: 0.9882 - val_loss: 1.0087 - val_accuracy: 0.7270\n",
      "Epoch 21/50\n",
      "6851/6851 [==============================] - 4s 566us/sample - loss: 0.0248 - accuracy: 0.9883 - val_loss: 1.0118 - val_accuracy: 0.7270\n",
      "Epoch 22/50\n",
      "6851/6851 [==============================] - 4s 567us/sample - loss: 0.0246 - accuracy: 0.9883 - val_loss: 1.0156 - val_accuracy: 0.7270\n",
      "Epoch 23/50\n",
      "6851/6851 [==============================] - 4s 561us/sample - loss: 0.0245 - accuracy: 0.9882 - val_loss: 1.0234 - val_accuracy: 0.7257\n",
      "Epoch 24/50\n",
      "6851/6851 [==============================] - 4s 565us/sample - loss: 0.0244 - accuracy: 0.9874 - val_loss: 1.0264 - val_accuracy: 0.7270\n",
      "Epoch 25/50\n",
      "6851/6851 [==============================] - 4s 569us/sample - loss: 0.0243 - accuracy: 0.9880 - val_loss: 1.0316 - val_accuracy: 0.7244\n",
      "Epoch 26/50\n",
      "6851/6851 [==============================] - 4s 594us/sample - loss: 0.0242 - accuracy: 0.9877 - val_loss: 1.0459 - val_accuracy: 0.7244\n",
      "Epoch 27/50\n",
      "6851/6851 [==============================] - 4s 651us/sample - loss: 0.0241 - accuracy: 0.9874 - val_loss: 1.0469 - val_accuracy: 0.7257\n",
      "Epoch 28/50\n",
      "6851/6851 [==============================] - 4s 611us/sample - loss: 0.0240 - accuracy: 0.9883 - val_loss: 1.0555 - val_accuracy: 0.7231\n",
      "Epoch 29/50\n",
      "6851/6851 [==============================] - 4s 586us/sample - loss: 0.0242 - accuracy: 0.9873 - val_loss: 1.0549 - val_accuracy: 0.7257\n",
      "Epoch 30/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9871\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "6851/6851 [==============================] - 4s 577us/sample - loss: 0.0241 - accuracy: 0.9872 - val_loss: 1.0650 - val_accuracy: 0.7231\n",
      "Epoch 31/50\n",
      "6851/6851 [==============================] - 4s 565us/sample - loss: 0.0232 - accuracy: 0.9882 - val_loss: 1.0654 - val_accuracy: 0.7231\n",
      "Epoch 32/50\n",
      "6851/6851 [==============================] - 4s 565us/sample - loss: 0.0232 - accuracy: 0.9886 - val_loss: 1.0654 - val_accuracy: 0.7244\n",
      "Epoch 33/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9888\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "6851/6851 [==============================] - 4s 585us/sample - loss: 0.0232 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 34/50\n",
      "6851/6851 [==============================] - 4s 563us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 35/50\n",
      "6851/6851 [==============================] - 4s 579us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0658 - val_accuracy: 0.7244\n",
      "Epoch 36/50\n",
      "6784/6851 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9889\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "6851/6851 [==============================] - 4s 575us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 37/50\n",
      "6851/6851 [==============================] - 4s 566us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 38/50\n",
      "6784/6851 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9889\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "6851/6851 [==============================] - 4s 565us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 39/50\n",
      "6851/6851 [==============================] - 4s 563us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 40/50\n",
      "6784/6851 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9888\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "6851/6851 [==============================] - 4s 564us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 41/50\n",
      "6851/6851 [==============================] - 4s 602us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 42/50\n",
      "6784/6851 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9888\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "6851/6851 [==============================] - 4s 596us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 43/50\n",
      "6851/6851 [==============================] - 4s 593us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 44/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9888\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "6851/6851 [==============================] - 4s 588us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 45/50\n",
      "6851/6851 [==============================] - 4s 567us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 46/50\n",
      "6816/6851 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9887\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "6851/6851 [==============================] - 4s 574us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "6851/6851 [==============================] - 4s 579us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 48/50\n",
      "6816/6851 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9887\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "6851/6851 [==============================] - 4s 565us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 49/50\n",
      "6851/6851 [==============================] - 4s 566us/sample - loss: 0.0231 - accuracy: 0.9888 - val_loss: 1.0659 - val_accuracy: 0.7244\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "#train m1\n",
    "history = model1.fit(\n",
    "    encoded_train,\n",
    "    label,\n",
    "    epochs = 50,\n",
    "    validation_split=0.1,\n",
    "    callbacks = callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now model2 use embedding layer and Bidirectional LSTM\n",
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 100, input_length = maxlen),\n",
    "    keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 28, 100)           1655200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,889,953\n",
      "Trainable params: 1,889,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/50\n",
      "6851/6851 [==============================] - 26s 4ms/sample - loss: 0.6749 - accuracy: 0.5779 - val_loss: 0.6716 - val_accuracy: 0.5538\n",
      "Epoch 2/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.6149 - accuracy: 0.6831 - val_loss: 0.5236 - val_accuracy: 0.7638\n",
      "Epoch 3/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.3989 - accuracy: 0.8342 - val_loss: 0.4436 - val_accuracy: 0.8031\n",
      "Epoch 4/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.3167 - accuracy: 0.8768 - val_loss: 0.4614 - val_accuracy: 0.8005\n",
      "Epoch 5/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.2553 - accuracy: 0.9096 - val_loss: 0.4920 - val_accuracy: 0.7848\n",
      "Epoch 6/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.2082 - accuracy: 0.9279 - val_loss: 0.5356 - val_accuracy: 0.7808\n",
      "Epoch 7/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.1757 - accuracy: 0.9407 - val_loss: 0.5735 - val_accuracy: 0.7717\n",
      "Epoch 8/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.1493 - accuracy: 0.9494 - val_loss: 0.6820 - val_accuracy: 0.7559\n",
      "Epoch 9/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.1288 - accuracy: 0.9593 - val_loss: 0.6821 - val_accuracy: 0.7677\n",
      "Epoch 10/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.1199 - accuracy: 0.9623 - val_loss: 0.6579 - val_accuracy: 0.7743\n",
      "Epoch 11/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.1050 - accuracy: 0.9686 - val_loss: 0.6420 - val_accuracy: 0.7703\n",
      "Epoch 12/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0987 - accuracy: 0.9683 - val_loss: 0.7103 - val_accuracy: 0.7690\n",
      "Epoch 13/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0910 - accuracy: 0.9714 - val_loss: 0.7052 - val_accuracy: 0.7572\n",
      "Epoch 14/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0823 - accuracy: 0.9739 - val_loss: 0.7491 - val_accuracy: 0.7690\n",
      "Epoch 15/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0799 - accuracy: 0.9755 - val_loss: 0.7109 - val_accuracy: 0.7585\n",
      "Epoch 16/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0778 - accuracy: 0.9764 - val_loss: 0.7313 - val_accuracy: 0.7638\n",
      "Epoch 17/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0734 - accuracy: 0.9765 - val_loss: 0.7070 - val_accuracy: 0.7730\n",
      "Epoch 18/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0670 - accuracy: 0.9785 - val_loss: 0.7584 - val_accuracy: 0.7664\n",
      "Epoch 19/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0663 - accuracy: 0.9797 - val_loss: 0.7927 - val_accuracy: 0.7769\n",
      "Epoch 20/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0623 - accuracy: 0.9788 - val_loss: 0.7841 - val_accuracy: 0.7717\n",
      "Epoch 21/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0583 - accuracy: 0.9807 - val_loss: 0.8106 - val_accuracy: 0.7743\n",
      "Epoch 22/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0594 - accuracy: 0.9799 - val_loss: 0.8128 - val_accuracy: 0.7717\n",
      "Epoch 23/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0555 - accuracy: 0.9818 - val_loss: 0.8361 - val_accuracy: 0.7677\n",
      "Epoch 24/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0559 - accuracy: 0.9822 - val_loss: 0.8176 - val_accuracy: 0.7717\n",
      "Epoch 25/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0529 - accuracy: 0.9828 - val_loss: 0.8119 - val_accuracy: 0.7769\n",
      "Epoch 26/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0530 - accuracy: 0.9838 - val_loss: 0.8379 - val_accuracy: 0.7703\n",
      "Epoch 27/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.8446 - val_accuracy: 0.7795\n",
      "Epoch 28/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0501 - accuracy: 0.9837 - val_loss: 0.8131 - val_accuracy: 0.7769\n",
      "Epoch 29/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0472 - accuracy: 0.9822 - val_loss: 0.8614 - val_accuracy: 0.7717\n",
      "Epoch 30/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0485 - accuracy: 0.9820 - val_loss: 0.8666 - val_accuracy: 0.7703\n",
      "Epoch 31/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.8707 - val_accuracy: 0.7782\n",
      "Epoch 32/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0479 - accuracy: 0.9828 - val_loss: 0.8747 - val_accuracy: 0.7756\n",
      "Epoch 33/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0438 - accuracy: 0.9842 - val_loss: 0.8918 - val_accuracy: 0.7730\n",
      "Epoch 34/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0424 - accuracy: 0.9850 - val_loss: 0.8413 - val_accuracy: 0.7913\n",
      "Epoch 35/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0443 - accuracy: 0.9853 - val_loss: 0.8996 - val_accuracy: 0.7743\n",
      "Epoch 36/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0415 - accuracy: 0.9851 - val_loss: 0.9814 - val_accuracy: 0.7690\n",
      "Epoch 37/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0424 - accuracy: 0.9841 - val_loss: 0.8803 - val_accuracy: 0.7874\n",
      "Epoch 38/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9842\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0433 - accuracy: 0.9842 - val_loss: 0.8749 - val_accuracy: 0.7795\n",
      "Epoch 39/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0376 - accuracy: 0.9858 - val_loss: 0.9046 - val_accuracy: 0.7822\n",
      "Epoch 40/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0346 - accuracy: 0.9869 - val_loss: 0.9305 - val_accuracy: 0.7808\n",
      "Epoch 41/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0352 - accuracy: 0.9873 - val_loss: 0.9490 - val_accuracy: 0.7808\n",
      "Epoch 42/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0344 - accuracy: 0.9863 - val_loss: 0.9550 - val_accuracy: 0.7795\n",
      "Epoch 43/50\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0354 - accuracy: 0.9858 - val_loss: 0.9618 - val_accuracy: 0.7795\n",
      "Epoch 44/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0322 - accuracy: 0.9873 - val_loss: 0.9748 - val_accuracy: 0.7782\n",
      "Epoch 45/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0347 - accuracy: 0.9853 - val_loss: 0.9796 - val_accuracy: 0.7782\n",
      "Epoch 46/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0330 - accuracy: 0.9880\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.9787 - val_accuracy: 0.7782\n",
      "Epoch 47/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0313 - accuracy: 0.9879 - val_loss: 0.9797 - val_accuracy: 0.7782\n",
      "Epoch 48/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0348 - accuracy: 0.9872 - val_loss: 0.9808 - val_accuracy: 0.7795\n",
      "Epoch 49/50\n",
      "6848/6851 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9864\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "6851/6851 [==============================] - 20s 3ms/sample - loss: 0.0336 - accuracy: 0.9864 - val_loss: 0.9817 - val_accuracy: 0.7795\n",
      "Epoch 50/50\n",
      "6851/6851 [==============================] - 19s 3ms/sample - loss: 0.0317 - accuracy: 0.9872 - val_loss: 0.9817 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "#train m2\n",
    "history = model2.fit(\n",
    "    encoded_train,\n",
    "    label,\n",
    "    epochs = 50,\n",
    "    validation_split=0.1,\n",
    "    callbacks = callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(encoded_test)\n",
    "pred2 = model2.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1rd=pred1.round().astype(np.int64)\n",
    "pred2rd=pred2.round().astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "sub1[\"target\"] = pred1rd\n",
    "sub1.to_csv(\"submit_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "sub2[\"target\"] = pred2rd\n",
    "sub2.to_csv(\"submit_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
